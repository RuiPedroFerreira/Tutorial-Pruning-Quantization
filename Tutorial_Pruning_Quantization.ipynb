{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial Pruning | Quantization**"
      ],
      "metadata": {
        "id": "DaN-4OkMbeLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developed by:\n",
        "*   Miguel Santos - M12960\n",
        "*   Rui Ferreira - M11911 "
      ],
      "metadata": {
        "id": "2zeT_Iptbf0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "University of Beira Interior (UBI)\n",
        "This tutorial is based on: \n",
        "\n",
        "*   https://github.com/christianversloot/machine-learning-articles/blob/main/tensorflow-model-optimization-an-introduction-to-pruning.md#loading-and-configuring-pruning\n",
        "\n",
        "* https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md\n",
        "\n",
        "* https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md"
      ],
      "metadata": {
        "id": "fHlIKEXWbl43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upgrade Tensorflow**"
      ],
      "metadata": {
        "id": "A7vI4zrdbnVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR-9DU11Qlo8",
        "outputId": "358348bb-e04a-470d-95d1-3fc9622e3e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the TensorFlow Model Optimization toolkit**"
      ],
      "metadata": {
        "id": "0est2CGXbr1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a collection for optimizing TensorFlow models, which minimizes the complexity of optimizing machine learning inference. For prunning, we will use this."
      ],
      "metadata": {
        "id": "gyk9qr_vbvRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/qkeras.git \n",
        "import sys \n",
        "sys.path.append('qkeras') \n",
        "!pip install git+https://github.com/keras-team/keras-tuner.git \n",
        "\n",
        "!pip install tensorflow_model_optimization \n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnuxbNNQQyqR",
        "outputId": "9beceba6-65c1-4f63-b3d1-b4a179f18323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qkeras'...\n",
            "remote: Enumerating objects: 2229, done.\u001b[K\n",
            "remote: Counting objects: 100% (761/761), done.\u001b[K\n",
            "remote: Compressing objects: 100% (309/309), done.\u001b[K\n",
            "remote: Total 2229 (delta 510), reused 658 (delta 439), pack-reused 1468\u001b[K\n",
            "Receiving objects: 100% (2229/2229), 1001.04 KiB | 13.17 MiB/s, done.\n",
            "Resolving deltas: 100% (1548/1548), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/keras-team/keras-tuner.git\n",
            "  Cloning https://github.com/keras-team/keras-tuner.git to /tmp/pip-req-build-0qd3buiw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/keras-team/keras-tuner.git /tmp/pip-req-build-0qd3buiw\n",
            "  Resolved https://github.com/keras-team/keras-tuner.git to commit 0dd114dd5353c941e8e14e68356a3fb124e3a0d1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner==1.3.5) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner==1.3.5) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner==1.3.5) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner==1.3.5) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner==1.3.5) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner==1.3.5) (3.4)\n",
            "Building wheels for collected packages: keras-tuner\n",
            "  Building wheel for keras-tuner (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.3.5-py3-none-any.whl size=176121 sha256=aed66dee70ae77bc8b5497d541a4a42617c066c3fcdecc4a852d875f98b30999\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7tjnon8/wheels/4a/30/da/29373541ae2fcd919cd8763ca501ac4c280c8024c2e38a10ed\n",
            "Successfully built keras-tuner\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.4-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Collecting numpy~=1.23\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.9/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Installing collected packages: numpy, tensorflow_model_optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.2 tensorflow_model_optimization-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the remaining necessary libraries.**"
      ],
      "metadata": {
        "id": "Hxs_rSVGcAuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras import models  \n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, InputLayer, Reshape, Conv2D, MaxPooling2D\n",
        "import tempfile\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "oP94RirIQzDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model configuration**"
      ],
      "metadata": {
        "id": "ACnR22vKcCOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell defines global variables for the code."
      ],
      "metadata": {
        "id": "qBdUORfrcM94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 28, 28\n",
        "batch_size = 64\n",
        "no_epochs = 10\n",
        "no_classes = 10\n",
        "validation_split = 0.2\n",
        "verbosity = 1"
      ],
      "metadata": {
        "id": "VpLoEwm-Q5gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load MNIST dataset**"
      ],
      "metadata": {
        "id": "npE6hi0Dcnrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used in this tutorial is MNIST dataset. The dataset contains 60,000 training images and 10,000 testing images of handwritten digits (0-9).\n",
        "\n",
        "In the case of MNIST dataset, the images are grayscale, meaning they have only one color channel. \n",
        "\n",
        "This line defines the shape of the input data for the ConvNet model. The img_width and img_height are the dimensions of each image, and 1 indicates that the images are grayscale."
      ],
      "metadata": {
        "id": "zOku2iJicm4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idA6nzgjS3IN",
        "outputId": "0d3b7de8-2dc3-4961-8ca5-dd04a3a992ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization Data**"
      ],
      "metadata": {
        "id": "hlDtZQJgczSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines normalize the input data by dividing each pixel value by 255, which scales the pixel values from the range of [0, 255] to the range of [0, 1].\n",
        "\n",
        "The normalization process is applied to each individual image in the training and testing sets separately."
      ],
      "metadata": {
        "id": "HMIfcfqvc3f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse numbers as floats\n",
        "input_train = train_images.astype('float32')\n",
        "input_test = test_images.astype('float32')\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "wp95yOfDQ6Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create and Training Model**"
      ],
      "metadata": {
        "id": "Gs-CHAkNdqPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    InputLayer(input_shape=(28, 28)),\n",
        "    #The added dimension represents the number of channels in the input tensor, which is 1 in this case since we are dealing with grayscale images.\n",
        "    Reshape(target_shape=(28, 28, 1)),\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(no_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit data to model\n",
        "model.fit(train_images, train_labels,\n",
        "          batch_size=batch_size,\n",
        "          epochs=no_epochs,\n",
        "          verbose=verbosity,\n",
        "          validation_split=validation_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlea4ZepRKPZ",
        "outputId": "6c9c4cea-ed2f-4c95-afec-f764edd9849d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "750/750 [==============================] - 18s 8ms/step - loss: 0.2944 - accuracy: 0.9113 - val_loss: 0.0929 - val_accuracy: 0.9724\n",
            "Epoch 2/10\n",
            "750/750 [==============================] - 6s 8ms/step - loss: 0.0974 - accuracy: 0.9704 - val_loss: 0.0645 - val_accuracy: 0.9826\n",
            "Epoch 3/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0727 - accuracy: 0.9764 - val_loss: 0.0545 - val_accuracy: 0.9840\n",
            "Epoch 4/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0640 - accuracy: 0.9801 - val_loss: 0.0444 - val_accuracy: 0.9868\n",
            "Epoch 5/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9879\n",
            "Epoch 6/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0482 - accuracy: 0.9854 - val_loss: 0.0385 - val_accuracy: 0.9889\n",
            "Epoch 7/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0414 - accuracy: 0.9872 - val_loss: 0.0363 - val_accuracy: 0.9890\n",
            "Epoch 8/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0404 - accuracy: 0.9874 - val_loss: 0.0341 - val_accuracy: 0.9900\n",
            "Epoch 9/10\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 0.0333 - accuracy: 0.9894 - val_loss: 0.0328 - val_accuracy: 0.9906\n",
            "Epoch 10/10\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.0341 - val_accuracy: 0.9901\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3ff537cfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: The Flatten layer in a neural network architecture is used to convert a multidimensional input tensor into a one-dimensional tensor that can be passed to the fully connected layers for classification."
      ],
      "metadata": {
        "id": "A3RqO86990lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Model**"
      ],
      "metadata": {
        "id": "Tjk37mvMeLg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate generalization metrics\n",
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMM5CepcTDJg",
        "outputId": "52705715-5ec1-4375-c904-6de0c6a57add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.030351150780916214 / Test accuracy: 0.9897000193595886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Model**"
      ],
      "metadata": {
        "id": "eIZvyXsweTVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "models.save_model(model, keras_file, include_optimizer=False)\n",
        "print(f'Baseline model saved: {keras_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWd1CfgJVrqh",
        "outputId": "f7a66482-3162-4269-a3a4-80954fe5450d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline model saved: /tmp/tmp7vyo3vso.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and Configuring PRUNING**"
      ],
      "metadata": {
        "id": "O4ZaeKY8edlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cells are responsible for adding prunning functionality to our code."
      ],
      "metadata": {
        "id": "T8YwzMmteevW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we use the **prune_low_magnitude** functionality ensure that model's layers are prunable, which means that it contains weights that can be safely removed without affecting the overall performance of the network. This only loads the functionality, we'll actually call it later."
      ],
      "metadata": {
        "id": "frje3Tl6emYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load functionality for adding pruning wrappers\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
      ],
      "metadata": {
        "id": "H0VYIdyfTQRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we are applying pruning to a Keras model using the TensorFlow Model Optimization (tfmot) library.\n",
        "\n",
        "\n",
        "*   **pruning_epochs**: number of epochs for which we want to apply pruning to the model.\n",
        "\n",
        "*   **num_images** : the number of images in the training set.\n",
        "\n",
        "*   **end_step** : Represents the step at which the pruning process will end during the training of a pruned model.\n",
        "\n",
        "*   **pruning_params** : Represent the configurations for the prunning operation. We define a pruning schedule using **PolynomialDecay**, which means that sparsity of the model increases with increasing number of epochs and to stop pruning at the specified end_step. Initially, we set the model to be 40% sparse, increasingly getting sparser to eventually 70%.\n",
        "\n",
        "* **pruning_schedule**: Specifies how the pruning rate changes over time during training. In this case, what is changing is the sparsity level.\n",
        "\n",
        "*   **model_for_prunning** : Thanks to the **prune_low_magnitude** functionality, the prunable model is generated taking into account our initial model and the prunning_params. It has the same architecture as the original model, but with some of the weights pruned according to the specified pruning schedule\n"
      ],
      "metadata": {
        "id": "-WprxmX38D1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finish pruning after 5 epochs\n",
        "pruning_epochs = 5\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * pruning_epochs\n",
        "\n",
        "# Define pruning configuration\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.40,\n",
        "                                                               final_sparsity=0.70,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# Recompile the model\n",
        "model_for_pruning.compile(loss=tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "5Unct9lxUF_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Starting Prunning Process**"
      ],
      "metadata": {
        "id": "z6DmOZuC7fGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After configuring the pruning process, we can actually recompile the model (this is necessary because we added pruning functionality), and start the pruning process. We must use the UpdatePruningStep callback here, because it  updates the pruning step after each epoch to ensure that the weights remain pruned in subsequent epochs.\n",
        "\n",
        "By finishing pruning after 5 epochs and using the defined pruning configuration, we can train a smaller model with fewer parameters and less memory usage. "
      ],
      "metadata": {
        "id": "o_8Os08I7eNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model callbacks\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep()\n",
        "]\n",
        "\n",
        "# Fitting data\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=pruning_epochs,\n",
        "                      verbose=verbosity,\n",
        "                      callbacks=callbacks,\n",
        "                      validation_split=validation_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvBOdqo9UdX5",
        "outputId": "688f5b72-388b-4bac-f0a5-36372bb16846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "750/750 [==============================] - 13s 10ms/step - loss: 0.0332 - accuracy: 0.9894 - val_loss: 0.0337 - val_accuracy: 0.9903\n",
            "Epoch 2/5\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.0308 - accuracy: 0.9905 - val_loss: 0.0342 - val_accuracy: 0.9901\n",
            "Epoch 3/5\n",
            "750/750 [==============================] - 7s 9ms/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.0337 - val_accuracy: 0.9898\n",
            "Epoch 4/5\n",
            "750/750 [==============================] - 5s 7ms/step - loss: 0.0305 - accuracy: 0.9898 - val_loss: 0.0369 - val_accuracy: 0.9893\n",
            "Epoch 5/5\n",
            "750/750 [==============================] - 7s 9ms/step - loss: 0.0293 - accuracy: 0.9900 - val_loss: 0.0314 - val_accuracy: 0.9913\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3ff4cf4b20>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Once pruning finishes, we must measure its effectiveness. We can do so in two ways:**\n",
        "\n",
        "*   By measuring how much performance has changed. compared to before pruning;\n",
        "*   By measuring how much model size has changed, compared to before pruning."
      ],
      "metadata": {
        "id": "PVOsCDZeftfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, there is minimal loss in test accuracy after pruning, compared to the baseline."
      ],
      "metadata": {
        "id": "g8j9l8L8fuUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate generalization metrics\n",
        "score_pruned = model_for_pruning.evaluate(test_images, test_labels, verbose=0) #Evaluate model\n",
        "\n",
        "print(f'Pruned CNN - Test loss: {score_pruned[0]} / Test accuracy: {score_pruned[1]}')\n",
        "print(f'Regular CNN - Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whnVPwmPUvj8",
        "outputId": "2748bff9-d480-4b4f-986b-90d7b985a1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned CNN - Test loss: 0.027494464069604874 / Test accuracy: 0.9908000230789185\n",
            "Regular CNN - Test loss: 0.030351150780916214 / Test accuracy: 0.9897000193595886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note: **\n",
        "The pruned model even performs slightly better than the regular one. This is likely because we trained the initial model for only 10 epochs, and subsequently continued with pruning afterwards. It's very much possible that the model had not yet converged; that moving towards convergence has continued in the pruning process. Often, performance deteriorates a bit, but should do so only slightly."
      ],
      "metadata": {
        "id": "kr7cHOqu7v6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a Keras model is pruned, some of its weights are set to zero to achieve sparsity, which can reduce the model's size and time of processing. However, the pruned model cannot be directly apply. In this way, we need first use **strip_pruning()** to removes the pruning operations and restores the original values of the weights that were pruned, thus returning the unpruned model. "
      ],
      "metadata": {
        "id": "0G_mU70FCeGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning) # Export prunning model. \n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print(f'Pruned model saved: {pruned_keras_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC1TMwEbU7p8",
        "outputId": "992e9db1-c1fd-4b9c-b7fa-d865b7a062d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model saved: /tmp/tmpcofv3h83.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measuring the size of your pruned model**"
      ],
      "metadata": {
        "id": "Jz_HfmTJe45E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bellow cell enables we verify how much the size of the model reduce with appplication of prunning.\n",
        "\n",
        "Define a helper function to actually compress the models via gzip and measure the zipped size."
      ],
      "metadata": {
        "id": "850g_ziJe6JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measuring the size of your pruned model\n",
        "# (source: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#fine-tune_pre-trained_model_with_pruning)\n",
        "\n",
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)\n"
      ],
      "metadata": {
        "id": "otVbZDb-VwVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-59qw3-XK5s",
        "outputId": "fc222a37-92e0-4115-f1d4-b714cfc84417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped baseline Keras model: 132445.00 bytes\n",
            "Size of gzipped pruned Keras model: 57389.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Ratio:{get_gzipped_model_size(keras_file)/get_gzipped_model_size(pruned_keras_file)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LqDzWtRChji",
        "outputId": "cbeab1a5-7f0a-45d6-b388-053d5892ff8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratio:2.307846451410549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Combining Pruning with Quantization for compound optimization**"
      ],
      "metadata": {
        "id": "fjIgFVbFfDcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization is another process that can be used to reduce the complexity and size of the model. It represents the number representation of the machine learning model (whether that's weights or also activations) in order to make it smaller."
      ],
      "metadata": {
        "id": "aNlhUMYifHsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **tensorflow.lite.Optimize.DEFAULT**: The default optimization strategy that enables post-training quantization. \n",
        "\n",
        "*   **TFLiteConverter**: Convert a Keras model into a TensorFlow Lite model, which is a format optimized for deployment on mobile and embedded devices.\n",
        "\n"
      ],
      "metadata": {
        "id": "paZhjrHZBZGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert Keras model into a TensorFlow Lite model \n",
        "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "\n",
        "converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\n",
        "\n",
        "#converts the Keras model to a quantized and pruned TensorFlow Lite model using the settings specified above.\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsclXhGBXFoc",
        "outputId": "07446e1d-b4c3-44a4-d0cb-abb87e33b3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved quantized and pruned TFLite model to: /tmp/tmpgo3z3bmk.tflite\n",
            "Size of gzipped baseline Keras model: 132445.00 bytes\n",
            "Size of gzipped pruned and quantized TFlite model: 19292.00 bytes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Ratio:{get_gzipped_model_size(keras_file)/get_gzipped_model_size(quantized_and_pruned_tflite_file)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehd7JpsDC9Pa",
        "outputId": "ef04e3c3-6df6-4ccd-8025-7a717b856d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratio:6.865280945469625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy from TF to TFLite**"
      ],
      "metadata": {
        "id": "DYrbQvKvfVPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a helper function to evaluate the TF Lite model on the test dataset."
      ],
      "metadata": {
        "id": "qG2SLI_ffZNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "CfD8FieiXMvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Accuracy of TF Lite model**"
      ],
      "metadata": {
        "id": "nWfzQSaZffiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tensorflow.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', score_pruned[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOYCgHmcXa-F",
        "outputId": "da0a9666-6bbb-45af-e8e7-b69fc72a5529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "Pruned and quantized TFLite test_accuracy: 0.9909\n",
            "Pruned TF test accuracy: 0.9908000230789185\n"
          ]
        }
      ]
    }
  ]
}
